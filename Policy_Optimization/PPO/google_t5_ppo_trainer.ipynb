{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install transformers==4.36.2\n",
    "!pip install datasets==2.15.0\n",
    "!pip install peft==0.7.1\n",
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install trl==0.7.7\n",
    "!pip install tqdm==4.66.1\n",
    "!pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
    "!pip install wandb"
   ],
   "id": "58cd94988228096f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from trl import PPOTrainer, PPOConfig, , create_reference_model, AutoModelForSeq2SeqLMWithValueHead\n",
    "from huggingface_hub import login\n",
    "import wandb"
   ],
   "id": "c9bef74b509e066b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "WANDB_TOKEN = user_secrets.get_secret(\"WANDB_TOKEN\")\n",
    "\n",
    "login(token = HF_TOKEN)\n",
    "wandb.login(key = WANDB_TOKEN)"
   ],
   "id": "e6029c6ad7d49efa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter Configuration",
   "id": "8799433a18d7ae4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"google-t5/t5-small\"\n",
    "reward_model_name = \"roberta-large-mnli\"\n",
    "\n",
    "batch_size= 64                  # Set the batch size for training.\n",
    "learning_rate=5e-5                 # Define the learning rate for the optimizer.\n",
    "remove_unused_columns=False        # Keep unused data columns in the training dataset.\n",
    "log_with=\"mlflow\"                  # Specify the logging method as \"mlflow\".\n",
    "gradient_accumulation_steps=4      # Number of gradient accumulation steps before updating the model.\n",
    "\n",
    "range_num = 30000\n",
    "N_EPOCHS = 1"
   ],
   "id": "c9ebdcb993a33f48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define generation hyperparameters\n",
    "\n",
    "# Set the minimum length of the generated output to 64 tokens.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 40,\n",
    "\n",
    "    # Configure the number of beams for beam search. Higher values lead to more diverse but slower generation.\n",
    "    \"num_beams\": 5,  # lookahead parameter\n",
    "\n",
    "    # Control the repetition of n-grams in the generated text. A value of 5 reduces repetitive phrases.\n",
    "    \"no_repeat_ngram_size\": 5,  # presence penalty\n",
    "\n",
    "    # Enable sampling during generation to introduce randomness in the output.\n",
    "    \"do_sample\": True,\n",
    "\n",
    "\n",
    "\n",
    "    # Set the maximum length of the generated output to 256 tokens.\n",
    "    \"max_length\": 512\n",
    "}\n",
    "extract_model_name = model_name.split(\"/\")[-1]\n",
    "extarct_reward_model_name = reward_model_name.replace(\"/\", \"-\")\n",
    "new_model_name = f\"obtained_model_name\""
   ],
   "id": "3fcebe3b479f055a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the base model",
   "id": "bf592b2b093b3d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Specify the padding token ID to use when generating sequences.\n",
    "generation_kwargs[\"pad_token_id\"] =  tokenizer.pad_token_id\n",
    "\n",
    "# Define the end-of-sequence token ID to signal the end of the generated text.\n",
    "generation_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id,\n",
    "\n",
    "# Model to fine-tune (T5 small)\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "# Reference model (Referans modelin tanımlanması)\n",
    "ref_model = create_reference_model(model)"
   ],
   "id": "c0486aeccb2bdb2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Prepration",
   "id": "2bb1532d5e7ba77d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = load_dataset(\"Muadil/all_cleaned_openai_summarize_comparisons_train_val\")[\"train\"]\n",
    "\n",
    "# Select new dataset\n",
    "dataset = dataset.select(range(range_num))\n",
    "\n",
    "dataset=dataset.map(process_func, batched=False)\n",
    "dataset.set_format(\"torch\")"
   ],
   "id": "21b9b164c9a55cbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PPO Configuration",
   "id": "6759165023444c7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# Reinforcement Learning Configuration\n",
    "\n",
    "# Create a configuration object for the PPO (Proximal Policy Optimization) algorithm.\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,           # Specify the name of the pre-trained model to use.\n",
    "    batch_size= batch_size,                  # Set the batch size for training.\n",
    "    learning_rate= learning_rate,                 # Define the learning rate for the optimizer.\n",
    "    remove_unused_columns= remove_unused_columns,        # Keep unused data columns in the training dataset.\n",
    "    log_with= log_with,                  # Specify the logging method as \"mlflow\".\n",
    "    gradient_accumulation_steps= gradient_accumulation_steps,      # Number of gradient accumulation steps before updating the model.\n",
    ")\n",
    "\n",
    "\n",
    "# Create a PPOTrainer instance for training a Proximal Policy Optimization (PPO) model.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,  # Configuration settings for the trainer.\n",
    "    model,  # The primary T5 model used for training.\n",
    "    ref_model,  # A reference T5 model for comparison or other purposes.\n",
    "    tokenizer,  # Tokenizer used to process input data.\n",
    "    dataset,  # Training dataset used to train the PPO model.\n",
    "    data_collator=collator  # Data collator for processing training data batches.\n",
    ")"
   ],
   "id": "7a48f91eda5243e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Reward Model",
   "id": "f45fe4c7bc11e3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model= reward_model_name)\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token"
   ],
   "id": "f85dfa7ce1f150e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PPO Training",
   "id": "3ca0d479254a7136"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:48:20.780041Z",
     "start_time": "2025-02-11T20:48:20.773506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "resultss = []\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epochs\"):\n",
    "    # We reset this list at the beginning of each epoch.\n",
    "    epoch_responses = []\n",
    "\n",
    "    for batch in tqdm(ppo_trainer.dataloader, desc=\"Batches\"):\n",
    "        # Create a dictionary to store game data for this batch\n",
    "        game_data = dict()\n",
    "\n",
    "        # Prepend the 'summarize:' token to each text in the batch\n",
    "        game_data[\"query\"] = batch[\"query\"]\n",
    "\n",
    "        # Generate responses from the updated t5 model\n",
    "        input_tensors = [_.squeeze() for _ in batch[\"input_ids\"]]\n",
    "        response_tensors = []\n",
    "\n",
    "        for query in input_tensors:\n",
    "            # Generate a response using PPO with specified generation parameters\n",
    "            response = ppo_trainer.generate(query.squeeze(), **generation_kwargs)\n",
    "            response_tensors.append(response.squeeze())\n",
    "\n",
    "        # Decode the response tensors to obtain the generated text\n",
    "        batch[\"response\"] = [\n",
    "            tokenizer.decode(r.squeeze(), skip_special_tokens=False)\n",
    "            for r in response_tensors\n",
    "        ]\n",
    "\n",
    "        #### Compute sentiment score\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = pipe(texts, **sent_kwargs)\n",
    "        resultss.append(pipe_outputs)\n",
    "        rewards = [torch.tensor(output[0]['score']) for output in pipe_outputs]\n",
    "\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(input_tensors, response_tensors, rewards)\n",
    "        if stats:\n",
    "            print(f\"Epoch {epoch} Stats:\")\n",
    "        else:\n",
    "            raise ValueError(\"PPO training step failed to return valid statistics.\")\n",
    "\n",
    "          # Let's choose some metrics that will be quickly understood by humans\n",
    "        meaningful_keys = [\n",
    "    \"train/mean_reward\",\n",
    "    \"objective/kl\",\n",
    "    \"loss/policy\",\n",
    "    \"loss/value\"\n",
    "]\n",
    "\n",
    "        print(\"\\n--- PPO Stats (Human-Readable) ---\")\n",
    "        for key in meaningful_keys:\n",
    "          if key in stats:\n",
    "        # Let's print the metric with, for example, 4 decimal places\n",
    "            print(f\"{key}: {stats[key]:.4f}\")\n",
    "        print(\"----------------------------------\\n\")\n",
    "\n"
   ],
   "id": "43fc66c252906270",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push to Hugging Face",
   "id": "48a9423176ca789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:48:21.410761Z",
     "start_time": "2025-02-11T20:48:21.407551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hugging Face Repository\n",
    "repo_name = f\"username/{new_model_name}\"\n",
    "\n",
    "# Save the model to Hugging Face Hub\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.com/{repo_name}\")"
   ],
   "id": "3464a792ef29f8fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
