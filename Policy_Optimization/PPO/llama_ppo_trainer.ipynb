{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Setting the working directory\n",
    "os.chdir('/kaggle/working/')\n",
    "os.system('rm -rf LLaMA-Factory')\n",
    "os.system('git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git')\n",
    "os.chdir('LLaMA-Factory')\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "!pip uninstall -q -y jax\n",
    "!pip install -q -e .[torch,bitsandbytes,liger-kernel]\n",
    "!pip uninstall wandb -y\n",
    "\n",
    "# Verify that CUDA is available\n",
    "try:\n",
    "    assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "    print(\"Please set up a GPU before using LLaMA Factory\")\n"
   ],
   "id": "46996eeda890428a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Kaggle Secrets Handling\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Hugging Face login\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ],
   "id": "43112f5f6e72bd5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter configurations",
   "id": "e054547b7f37eeab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stage = \"ppo\"\n",
    "data_size = 40000\n",
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "dataset_name = \"Muadil/ppo_datasets_summary\"\n",
    "reward_model_name = \"/kaggle/input/adapter/saves/skywork_llama/lora/reward\"\n",
    "batch_size = 4\n",
    "epoch_size = 1\n",
    "\n",
    "extract_reward_model_name = \"Skywork\""
   ],
   "id": "7a784d635dbcb824"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loading the Hugging Face dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Selecting the first 1k data\n",
    "\n",
    "resized_dataset = dataset[\"train\"].select(range(min(int(data_size*1.25), len(dataset[\"train\"]))))\n",
    "print(f\"Size of the data set: {len(resized_dataset)}\")\n",
    "\n",
    "# Split the dataset into 80% (train) and 20% (eval)\n",
    "train_test_split = resized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Separate Train and Eval sets\n",
    "train_dataset = train_test_split[\"train\"][\"prompt\"]\n",
    "eval_dataset = train_test_split[\"test\"][\"prompt\"]\n",
    "# Save to JSON file\n",
    "\n",
    "output_file = f\"data/{stage}_dataset_{data_size}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "output_file = f\"data/{stage}_eval_dataset_{data_size}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "import json\n",
    "\n",
    "# Read dataset file\n",
    "with open('data/dataset_info.json', 'r', encoding='utf-8') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# New data\n",
    "new_data = {\n",
    "     f\"{stage}_dataset_{data_size}\": {\n",
    "    \"file_name\": f\"{stage}_dataset_{data_size}.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  }\n",
    " }\n",
    "\n",
    "new_eval_data = {\n",
    "     f\"{stage}_eval_dataset_{data_size}\": {\n",
    "    \"file_name\": f\"{stage}_eval_dataset_{data_size}.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  }\n",
    " }\n",
    "# Add new data to existing file\n",
    "dataset_info.update(new_data)\n",
    "dataset_info.update(new_eval_data)\n",
    "\n",
    "# Save back updated data\n",
    "with open('data/dataset_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"The new data was successfully added and the file was saved.\")\n",
    "\n",
    "print(f\"The first {data_size} of data was successfully saved to {output_file}.\")"
   ],
   "id": "eea6f552009cbe99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "  stage=stage,\n",
    "  quantization_bit=4, #nf4 to do quantization, default is None so it does not quantize.\n",
    "  quantization_method=\"bitsandbytes\", # we enter in which format to quantize. bitsandbytes, hqq, eetq, etc.\n",
    "  do_train=True,\n",
    "  model_name_or_path= base_model, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=f\"{stage}_dataset_{data_size}\",             # use alpaca and identity datasets\n",
    "    #eval_dataset = f\"{stage}_eval_dataset_{data_size}\",\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  reward_model = reward_model_name,\n",
    "    output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=batch_size,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                     # the learning rate\n",
    "  num_train_epochs=epoch_size,                    # the epochs of training\n",
    "  max_samples=500,                      # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                         # use float16 mixed precision training\n",
    "  enable_liger_kernel=True,                   # use liger kernel for efficient training\n",
    "  #report_to=\"wandb\",\n",
    "  run_name=f\"username/repository\",\n",
    "  #load_best_model_at_end=True,  # Ensure the best model is loaded at the end\n",
    "  metric_for_best_model=\"eval_loss\",  # Metric to monitor for the best model\n",
    "  greater_is_better=False,\n",
    "    #evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "# Start the training process\n",
    "!llamafactory-cli train train_llama3.json"
   ],
   "id": "f51033db17a0cd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Saving and exporting the model\n",
    "export_args = dict(\n",
    "    model_name_or_path=base_model,  # original model\n",
    "    adapter_name_or_path=\"llama3_lora\",  # saved LoRA adapters\n",
    "    template=\"llama3\",  # same template\n",
    "    finetuning_type=\"lora\",  # use of the same LoRA\n",
    "    export_dir=\"llama3_lora_merged\",  # where to register the unified model\n",
    "    export_size=5,  # model file size (GB)\n",
    "    export_device=\"cpu\",  # export device\n",
    "    export_hub_model_id= \"username/repository\",  # Hugging Face model ID\n",
    ")\n",
    "\n",
    "# Export the model and upload it to Hugging Face\n",
    "\n",
    "json.dump(export_args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "!llamafactory-cli export merge_llama3.json"
   ],
   "id": "1d3115db6ff3053"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
