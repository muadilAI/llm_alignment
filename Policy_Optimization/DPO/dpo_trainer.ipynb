{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6218bedb6363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the specified version of the Hugging Face Hub package\n",
    "!pip install -U huggingface-hub==0.27.1  \n",
    "\n",
    "import wandb  \n",
    "\n",
    "# Handling Kaggle Secrets  \n",
    "from kaggle_secrets import UserSecretsClient  \n",
    "user_secrets = UserSecretsClient()  \n",
    "\n",
    "# Retrieve API tokens from Kaggle secrets  \n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")  \n",
    "wb_token = user_secrets.get_secret(\"WANDB_TOKEN\")  \n",
    "\n",
    "# Login to Weights & Biases  \n",
    "wandb.login(key=wb_token)  \n",
    "\n",
    "# Login to Hugging Face Hub  \n",
    "from huggingface_hub import login  \n",
    "login(token=hf_token)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa49c3b2e859b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface-hub==0.27.1 transformers==4.46.1 datasets==3.1.0 torch==2.5.1+cu121 bitsandbytes==0.45.0\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/kaggle/working/')\n",
    "os.system('rm -rf LLaMA-Factory')  # Remove any existing LLaMA-Factory directory\n",
    "os.system('git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git')  # Clone the repository\n",
    "os.chdir('LLaMA-Factory')\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "!pip uninstall -q -y jax  # Uninstall JAX if present\n",
    "!pip install -q -e .[torch,bitsandbytes,liger-kernel]  # Install LLaMA-Factory dependencies\n",
    "\n",
    "# Verify CUDA availability\n",
    "try:\n",
    "    assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "    print(\"Please set up a GPU before using LLaMA Factory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c95aaa4ec5e3e2",
   "metadata": {},
   "source": [
    "# Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ee4c201f81fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"dpo\"  # Training stage: Direct Preference Optimization (DPO)\n",
    "data_size = 20000  # Number of training samples\n",
    "base_model = \"Llama-3.2-1B-Instruct\"  # Pretrained base model used for fine-tuning\n",
    "dataset_name = \"Muadil/dpo_dataset_train_openai_summary\"  # Dataset path in Hugging Face Datasets\n",
    "batch_size = 2  # Number of samples per training batch\n",
    "epoch_size = 3  # Number of training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49119f54639d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Hugging Face dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Selecting the first x data points\n",
    "resized_dataset = dataset[\"train\"].select(range(min(int(data_size * 1.25), len(dataset[\"train\"]))))\n",
    "\n",
    "# Splitting the dataset into 80% training and 20% evaluation\n",
    "train_test_split = resized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Extracting training and evaluation sets\n",
    "train_dataset = train_test_split[\"train\"][\"prompt\"]\n",
    "eval_dataset = train_test_split[\"test\"][\"prompt\"]\n",
    "\n",
    "# Saving datasets to JSON files\n",
    "train_output_file = f\"data/{stage}_dataset_{data_size}.json\"\n",
    "with open(train_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "eval_output_file = f\"data/{stage}_eval_dataset_{data_size}.json\"\n",
    "with open(eval_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "import json\n",
    "\n",
    "# Loading the dataset information file\n",
    "with open(\"data/dataset_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# Creating new dataset entries\n",
    "new_data = {\n",
    "    f\"{stage}_dataset_{data_size}\": {\n",
    "        \"file_name\": f\"{stage}_dataset_{data_size}.json\",\n",
    "        \"ranking\": True,\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"chosen\": \"chosen\",\n",
    "            \"rejected\": \"rejected\",\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "new_eval_data = {\n",
    "    f\"{stage}_eval_dataset_{data_size}\": {\n",
    "        \"file_name\": f\"{stage}_eval_dataset_{data_size}.json\",\n",
    "        \"ranking\": True,\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"chosen\": \"chosen\",\n",
    "            \"rejected\": \"rejected\",\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Updating dataset information with new entries\n",
    "dataset_info.update(new_data)\n",
    "dataset_info.update(new_eval_data)\n",
    "\n",
    "# Saving the updated dataset information\n",
    "with open(\"data/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"New dataset entries have been successfully added and saved.\")\n",
    "\n",
    "print(f\"The first {data_size} data points have been successfully saved to {train_output_file} and {eval_output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1d5e00066c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "args = dict(\n",
    "  quantization_bit=4, #nf4 to do quantization, default is None so it does not quantize.\n",
    "  quantization_method=\"bitsandbytes\", # we enter in which format to quantize. bitsandbytes, hqq, eetq, etc.\n",
    "  stage=stage,\n",
    "  do_train=True,\n",
    "  pref_loss = \"sigmoid\",\n",
    "  model_name_or_path=f\"meta-llama/{base_model}\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=f\"{stage}_dataset_{data_size}\",             # use alpaca and identity datasets\n",
    "  #eval_dataset = f\"{stage}_eval_dataset_{data_size}\",\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=batch_size,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                     # the learning rate\n",
    "  num_train_epochs=epoch_size,                    # the epochs of training\n",
    "  max_samples=500,                      # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                         # use float16 mixed precision training\n",
    "  enable_liger_kernel=True,                   # use liger kernel for efficient training\n",
    "  report_to=\"wandb\",\n",
    "  run_name=\"username/repository\",\n",
    "  #load_best_model_at_end=True,  # Ensure the best model is loaded at the end\n",
    "  metric_for_best_model=\"eval_loss\",  # Metric to monitor for the best model\n",
    "  greater_is_better=False,\n",
    "  #evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "# Start the training process\n",
    "!llamafactory-cli train train_llama3.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cb43217e3a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and exporting the model\n",
    "export_args = {\n",
    "    \"model_name_or_path\": f\"meta-llama/{base_model}\",  # Original model\n",
    "    \"adapter_name_or_path\": \"llama3_lora\",  # Saved LoRA adapters\n",
    "    \"template\": \"llama3\",  # Same template\n",
    "    \"finetuning_type\": \"lora\",  # Using the same LoRA method\n",
    "    \"export_dir\": \"llama3_lora_merged\",  # Directory to store the merged model\n",
    "    \"export_size\": 5,  # Model file size (GB)\n",
    "    \"export_device\": \"cpu\",  # Device for exporting\n",
    "    \"export_hub_model_id\": \"username/repository\",  # Hugging Face model ID\n",
    "}\n",
    "\n",
    "# Save export arguments to a JSON file\n",
    "with open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_args, f, indent=2)\n",
    "\n",
    "# Export the model and upload it to Hugging Face\n",
    "!llamafactory-cli export merge_llama3.json\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
