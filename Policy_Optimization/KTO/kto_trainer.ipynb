{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6218bedb6363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required version of Hugging Face Hub\n",
    "!pip install -U huggingface-hub==0.27.1\n",
    "\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Initialize Kaggle Secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Retrieve API tokens from Kaggle Secrets\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "wb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n",
    "\n",
    "# Log in to Weights & Biases (W&B)\n",
    "wandb.login(key=wb_token)\n",
    "\n",
    "# Log in to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa49c3b2e859b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies (if needed)\n",
    "# !pip install -U huggingface-hub==0.27.1 transformers==4.46.1 datasets==3.1.0 torch==2.5.1+cu121 bitsandbytes==0.45.0\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir('/kaggle/working/')\n",
    "\n",
    "# Remove any existing LLaMA-Factory directory and clone a fresh copy\n",
    "os.system('rm -rf LLaMA-Factory')\n",
    "os.system('git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git')\n",
    "os.chdir('LLaMA-Factory')\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "!pip uninstall -q -y jax\n",
    "!pip install -q -e .[torch,bitsandbytes,liger-kernel]\n",
    "\n",
    "# Verify if CUDA is available\n",
    "try:\n",
    "    assert torch.cuda.is_available()\n",
    "except AssertionError:\n",
    "    print(\"Please set up a GPU before using LLaMA Factory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c95aaa4ec5e3e2",
   "metadata": {},
   "source": [
    "# Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ee4c201f81fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"kto\"  # Training stage: Kahneman-Tversky Optimization (KTO)\n",
    "data_size = 40000  # Number of training samples\n",
    "base_model = \"Llama-3.2-1B-Instruct\"  # Pretrained base model used for fine-tuning\n",
    "dataset_name = \"Muadil/kto_labeled_openai_summary\"  # Dataset path in Hugging Face Datasets\n",
    "batch_size = 8  # Number of samples per training batch\n",
    "epoch_size = 1  # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49119f54639d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from Hugging Face\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Selecting the first 1,000 data points (or adjusted size based on data_size)\n",
    "resized_dataset = dataset[\"train\"].select(range(min(int(data_size*1.25), len(dataset[\"train\"]))))\n",
    "\n",
    "# Splitting the dataset into training (80%) and evaluation (20%) sets\n",
    "train_test_split = resized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Extracting the \"prompt\" column for training and evaluation datasets\n",
    "train_dataset = train_test_split[\"train\"][\"prompt\"]\n",
    "eval_dataset = train_test_split[\"test\"][\"prompt\"]\n",
    "\n",
    "# Saving the training dataset to a JSON file\n",
    "output_file = f\"data/{stage}_dataset_{data_size}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Saving the evaluation dataset to a JSON file\n",
    "output_file = f\"data/{stage}_eval_dataset_{data_size}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "import json\n",
    "\n",
    "# Reading the dataset information file to update it\n",
    "with open('data/dataset_info.json', 'r', encoding='utf-8') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# Adding new data for training dataset\n",
    "new_data = {\n",
    "    f\"{stage}_dataset_{data_size}\": {\n",
    "        \"file_name\": f\"{stage}_dataset_{data_size}.json\",\n",
    "        \"formatting\": \"sharegpt\",  # Format used for the data\n",
    "        \"columns\": {\n",
    "            \"messages\": \"messages\",  # Column representing the messages\n",
    "            \"kto_tag\": \"label\"  # Column representing the labels for classification\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"role_tag\": \"role\",  # Tag for the role in the conversation\n",
    "            \"content_tag\": \"content\",  # Tag for the message content\n",
    "            \"user_tag\": \"user\",  # Tag for user-related content\n",
    "            \"assistant_tag\": \"assistant\"  # Tag for assistant-related content\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adding new evaluation data\n",
    "new_eval_data = {\n",
    "    f\"{stage}_eval_dataset_{data_size}\": {\n",
    "        \"file_name\": f\"{stage}_dataset_{data_size}.json\",\n",
    "        \"formatting\": \"sharegpt\",  # Format used for the data\n",
    "        \"columns\": {\n",
    "            \"messages\": \"messages\",  # Column representing the messages\n",
    "            \"kto_tag\": \"label\"  # Column representing the labels for classification\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"role_tag\": \"role\",  # Tag for the role in the conversation\n",
    "            \"content_tag\": \"content\",  # Tag for the message content\n",
    "            \"user_tag\": \"user\",  # Tag for user-related content\n",
    "            \"assistant_tag\": \"assistant\"  # Tag for assistant-related content\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Updating the existing dataset information with new data\n",
    "dataset_info.update(new_data)\n",
    "dataset_info.update(new_eval_data)\n",
    "\n",
    "# Saving the updated dataset information back to the file\n",
    "with open('data/dataset_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Outputting a success message\n",
    "print(\"The new data was successfully added and the file was saved.\")\n",
    "\n",
    "# Outputting the saved dataset file path\n",
    "print(f\"The first {data_size} of data was successfully saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1d5e00066c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "args = dict(\n",
    "  quantization_bit=4, #nf4 to do quantization, default is None so it does not quantize.\n",
    "  quantization_method=\"bitsandbytes\", # we enter in which format to quantize. bitsandbytes, hqq, eetq, etc.\n",
    "  stage=stage,\n",
    "  # kto_chosen_weight = 1, # default value\n",
    "  # pref_beta = 0.1, # default value\n",
    "  kto_rejected_weight = 1.33,\n",
    "  do_train=True,\n",
    "  model_name_or_path=f\"meta-llama/{base_model}\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=f\"{stage}_dataset_{data_size}\",             # use alpaca and identity datasets\n",
    "  # eval_dataset = f\"{stage}_eval_dataset_{data_size}\",\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=batch_size,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                     # the learning rate\n",
    "  num_train_epochs=epoch_size,                    # the epochs of training\n",
    "  max_samples=500,                      # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                         # use float16 mixed precision training\n",
    "  enable_liger_kernel=True,                   # use liger kernel for efficient training\n",
    "  # report_to=\"wandb\",\n",
    "  run_name=f\"Muadil/{base_model}_sum_{int(data_size/1000)}k_{batch_size}_{epoch_size}ep\",\n",
    "  # load_best_model_at_end=True,  # Ensure the best model is loaded at the end\n",
    "  # metric_for_best_model=\"eval_loss\",  # Metric to monitor for the best model\n",
    "  greater_is_better=False,\n",
    "  # evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "# Start the training process\n",
    "!llamafactory-cli train train_llama3.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cb43217e3a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and exporting the model\n",
    "export_args = dict(\n",
    "    model_name_or_path=f\"meta-llama/{base_model}\",  # Path to the original base model\n",
    "    adapter_name_or_path=\"llama3_lora\",  # Path to the saved LoRA adapters\n",
    "    template=\"llama3\",  # Template used for the model (keeps the same format)\n",
    "    finetuning_type=\"lora\",  # Specifies the type of fine-tuning (using LoRA)\n",
    "    export_dir=\"llama3_lora_merged\",  # Directory where the merged model will be saved\n",
    "    export_size=5,  # Size of the model file in GB\n",
    "    export_device=\"cpu\",  # The device where the model will be exported (in this case, CPU)\n",
    "    export_hub_model_id=\"username/repository\",  # Hugging Face model repository ID for uploading\n",
    ")\n",
    "\n",
    "# Exporting the model by saving the configuration to a JSON file\n",
    "json.dump(export_args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "# Using the LLaMA Factory CLI tool to export the model with the provided configuration\n",
    "!llamafactory-cli export merge_llama3.json\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
