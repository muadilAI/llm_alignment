{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install necessary libraries\n",
    "!pip install inflect\n",
    "!pip install -q datasets\n",
    "!pip install --upgrade huggingface_hub"
   ],
   "id": "f837bf6925e449b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import re\n",
    "import inflect\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ],
   "id": "4ed7bccedb902b30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Authenticate with Hugging Face Hub\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "login(token=HF_TOKEN)"
   ],
   "id": "1ac4513f5883d50c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def character_repetition(text):\n",
    "    \"\"\"\n",
    "    Limits excessive character repetitions in a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with excessive character and punctuation repetitions removed.\n",
    "    \"\"\"\n",
    "    # Replace excessive letter repetitions (more than two)\n",
    "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{2,}\", re.DOTALL)\n",
    "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # Replace excessive punctuation repetitions (more than one)\n",
    "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    cleaned_text = pattern_punct.sub(r'\\1', formatted_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_from_openai_summarize_comparisons_dataset(row):\n",
    "    \"\"\"\n",
    "    Extracts post content, subreddit, and title from a dataset row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series containing the extracted post content, subreddit, and title.\n",
    "    \"\"\"\n",
    "    category_post = clean_text(row[\"prompt\"]).split(\"POST\", 1)\n",
    "\n",
    "    # If \"POST\" keyword is missing, return original text with unknown labels\n",
    "    if len(category_post) < 2:\n",
    "        return pd.Series([row[\"prompt\"], \"unknown\", \"unknown\"])\n",
    "\n",
    "    # Extract the actual post content\n",
    "    post = category_post[1]\n",
    "    post = re.sub(r'\\r', ' ', post)  # Replace carriage return characters with spaces\n",
    "    post = character_repetition(post)\n",
    "\n",
    "    # Extract subreddit and title\n",
    "    raw_category = category_post[0]\n",
    "    subreddit_match = re.search(r'SUBREDDIT\\s+(.*?)(?=\\s+TITLE)', raw_category)\n",
    "    title_match = re.search(r'TITLE\\s*(.*)', raw_category)\n",
    "\n",
    "    subreddit_value = subreddit_match.group(1).strip().lower() if subreddit_match else None\n",
    "    title_value = title_match.group(1).strip().lower() if title_match else None\n",
    "\n",
    "    return pd.Series([post, subreddit_value, title_value])\n"
   ],
   "id": "7f86bddc31c3a8e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remove_tldr(text):\n",
    "    \"\"\"\n",
    "    Removes 'TL;DR: ' phrase from the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    return text.replace(\"TL;DR: \", \"\")\n",
    "\n",
    "def remove_citizens_for_the_republic(text):\n",
    "    \"\"\"\n",
    "    Removes 'Citizens for the Republic' phrase from the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    return text.replace(\"Citizens for the Republic\", \"\")\n",
    "\n",
    "def remove_html_tags(sentence):\n",
    "    \"\"\"\n",
    "    Removes HTML tags from the given text.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned sentence without HTML tags.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"<.*?>\")\n",
    "    return re.sub(pattern, '', sentence).strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters, HTML tags, and converting numbers to words.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = remove_html_tags(text)\n",
    "    text = re.sub(r'\\n|\\r', ' ', text)  # Replace new lines with space\n",
    "    text = re.sub(r'[#]', '', text)  # Remove hash characters\n",
    "    text = re.sub(r'\\br/\\b', '', text)  # Remove \"r/\" prefix\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text inside square brackets\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "\n",
    "    # Convert numbers to words\n",
    "    def number_to_words(match):\n",
    "        p = inflect.engine()\n",
    "        return p.number_to_words(int(match.group(0)))\n",
    "\n",
    "    text = re.sub(r'\\b\\d+\\b', number_to_words, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_from_openai_summarize_comparisons(dataset_names=None):\n",
    "    \"\"\"\n",
    "    Loads dataset splits from Hugging Face and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataset_names (list, optional): List of dataset splits (e.g., [\"train\", \"test\"]). Defaults to all.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with 'prompt', 'chosen', and 'rejected' columns.\n",
    "    \"\"\"\n",
    "    if dataset_names is None:\n",
    "        dataset_names = [\"train\", \"test\", \"valid1\", \"valid2\"]\n",
    "\n",
    "    splits = {\n",
    "        'train': 'data/train-00000-of-00001-3cbd295cedeecf91.parquet',\n",
    "        'test': 'data/test-00000-of-00001-0845e2eec675b16a.parquet',\n",
    "        'valid1': 'data/valid1-00000-of-00001-b647616a2be5f333.parquet',\n",
    "        'valid2': 'data/valid2-00000-of-00001-2655c5b3621b6116.parquet'\n",
    "    }\n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "        if dataset_name not in splits:\n",
    "            raise ValueError(f\"Invalid dataset name: {dataset_name}. Choose from {list(splits.keys())}.\")\n",
    "\n",
    "    df_raw = pd.DataFrame()\n",
    "    for dataset_name in dataset_names:\n",
    "        df_temp = pd.read_parquet(\"hf://datasets/CarperAI/openai_summarize_comparisons/\" + splits[dataset_name])\n",
    "        df_raw = pd.concat([df_raw, df_temp])\n",
    "\n",
    "    return df_raw\n",
    "\n",
    "def preprocess_openai_summarize_comparisons_dataset(df, topic_list=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset by extracting structured information and cleaning the summaries.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        topic_list (list, optional): List of topics to filter. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset with selected columns.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[[\"prompt\", \"topic\", \"title\"]] = df.apply(extract_from_openai_summarize_comparisons_dataset, axis=1)\n",
    "\n",
    "    for column in [\"chosen\", \"rejected\"]:\n",
    "        df2[column] = df2[column].apply(clean_text).apply(remove_tldr).apply(remove_citizens_for_the_republic)\n",
    "\n",
    "    if topic_list is None:\n",
    "        return df2[[\"prompt\", \"chosen\", \"rejected\"]]\n",
    "\n",
    "    df2 = df2[df2[\"topic\"].isin(topic_list)]\n",
    "\n",
    "    return df2[[\"prompt\", \"chosen\", \"rejected\"]]\n",
    "\n",
    "def plot_topic_distribution(df):\n",
    "    \"\"\"\n",
    "    Plots the distribution of topics in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'topic' column.\n",
    "    \"\"\"\n",
    "    df[\"topic\"].value_counts().plot(kind=\"barh\", figsize=(10, 6))\n",
    "    plt.title(\"Topic Distribution\", fontsize=14)\n",
    "    plt.xlabel(\"Frequency\", fontsize=12)\n",
    "    plt.ylabel(\"Topic\", fontsize=12)\n",
    "    plt.savefig(\"topic_distribution.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def upload_dataset_to_huggingface(dataset, repo_name, token):\n",
    "    \"\"\"\n",
    "    Hugging Face Hub'a Dataset yükleme fonksiyonu.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset objesi.\n",
    "        repo_name (str): Hugging Face repository ismi.\n",
    "        token (str): Hugging Face erişim tokeni.\n",
    "        private (bool): Repository'nin public/private ayarı. Varsayılan: False (public).\n",
    "\n",
    "    Returns:\n",
    "        str: Repository URL.\n",
    "    \"\"\"\n",
    "    # Dataset'i Hugging Face Hub'a push etmek\n",
    "    dataset.push_to_hub(repo_id=repo_name)\n",
    "\n",
    "    print(f\"Dataset '{repo_name}' successfully uploaded to Hugging Face Hub!\")\n",
    "    return f\"https://huggingface.co/datasets/{repo_name}\""
   ],
   "id": "527ce1bb1d873a64",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = get_from_openai_summarize_comparisons([\"train\", \"valid1\", \"valid2\"])",
   "id": "fa4dea4245cac473"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()",
   "id": "a23571ec3afafe2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.value_counts(\"topic\")",
   "id": "f724bb145110eadb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Topic Distribution Visualization\n",
    "plot_topic_distribution(df)"
   ],
   "id": "948386fb5515e2b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = preprocess_openai_summarize_comparisons_dataset(df.loc[:100], [\"relationships\"])",
   "id": "89685cd4d25e8cc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()",
   "id": "a47c1106528a55b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Enter token and file information\n",
    "pkl_file_path = \"dataset.pkl\"  # Path to the .pkl file to upload\n",
    "repo_name = \"username/repo_name\"  # Hugging Face repository name\n",
    "# Upload Dataset to Hugging Face Hub\n",
    "upload_dataset_to_huggingface(df, repo_name, HF_TOKEN)"
   ],
   "id": "63c0babfb747c6f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
